{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d968d508-e68b-473e-8250-a7efd8d898a1",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f06b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models, datasets\n",
    "import torch.utils.data as data\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c370c5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "    transforms.Resize((80, 80)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    \n",
    "    'val': transforms.Compose([\n",
    "    transforms.Resize((80, 80)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    \n",
    "    'test': transforms.Compose([\n",
    "    transforms.Resize((80, 80)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])\n",
    "} #resizing and normalizing images\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 5\n",
    "num_epochs = 15\n",
    "#hyperparameters\n",
    "\n",
    "data_dir = 'TrafficSignDataset'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val', 'test']}\n",
    "#loading data\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1904fbc",
   "metadata": {},
   "source": [
    "## Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = std * inp + mean\n",
    "    plt.imshow(inp)\n",
    "    plt.title( 'Samples of Training Images')\n",
    "    plt.show()\n",
    "\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "print('All Classes: ', class_names)\n",
    "\n",
    "imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d0364",
   "metadata": {},
   "source": [
    "## Network Model\n",
    "### (CNN to FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 9) \n",
    "        self.conv2 = nn.Conv2d(6, 10, 5)\n",
    "        self.conv3 = nn.Conv2d(10, 16, 3)\n",
    "        self.conv4 = nn.Conv2d(16, 20, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(20 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN layer\n",
    "        x = self.conv1(x) # 80*80*3 -> 72*72*6      \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # -> 36*36*6\n",
    "        x = self.conv2(x) # -> 32*32*10\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # ->16*16*10\n",
    "        x = self.conv3(x) # -> 14*14*16\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # -> 7*7*16\n",
    "        x = self.conv4(x) # -> 5*5*20\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # FCNN layer\n",
    "        x = x.view(-1, 20 * 5 * 5) # flatten\n",
    "        x = self.fc1(x) # 500(5*5*20) -> 120\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x) # 120 -> 80\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x) # 80 -> 5 classes\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969e35a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86667db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, num_epochs):\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(\"Training phase:\")\n",
    "    print()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-------------')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            total_loss = 0.0\n",
    "            corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "            \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                corrects += (preds == labels).sum()\n",
    "                \n",
    "            epoch_average_loss = total_loss / (dataset_sizes[phase]/batch_size)\n",
    "            epoch_acc = corrects / dataset_sizes[phase]\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_average_loss:.4f} Acc:{100 * epoch_acc:.4f}%')\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "\n",
    "        print()\n",
    "        \n",
    "    print(f'Best val Acc: {100 * best_acc:4f}%')\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed/60:.0f}min and {time_elapsed%60:.0f}s')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd63094",
   "metadata": {},
   "source": [
    "### Calling the training model and loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b368a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model = train_model(model, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d728b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(dataset_sizes['test'])]\n",
    "    n_class_samples = [0 for i in range(dataset_sizes['test'])]\n",
    "    for images, labels in dataloaders['test']:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum()\n",
    "        for i in range (batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            \n",
    "    acc = 100.0 * n_correct.item()/n_samples\n",
    "    print(\"Testing phase:\")\n",
    "    print()\n",
    "    print('============================================')\n",
    "    print(f\"Accuracy of the network: {acc}%\")\n",
    "    print('============================================')\n",
    "    print()\n",
    "    print('--------------------------------------------')\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        acc = 100.0 * n_class_correct[i]/n_class_samples[i]\n",
    "        print(f\"Accuracy of {class_names[i]}: {acc}%\")\n",
    "        print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64814a6b",
   "metadata": {},
   "source": [
    "#### Change this cell to code to save your weights\n",
    "\n",
    "PATH = './tsr_wts.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
